#+TITLE: IsaSAT
#+AUTHOR: Mathias Fleury
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>



As part of my PhD thesis, I developed a framework to formalize SAT solving. I still continue
to expend it. The
general organization is given by the following graph:
[[file:../figures/sat/sat_framework.png]]

* SAT Solver Framework
:PROPERTIES:
:CUSTOM_ID: h:sat-solver-framework
:END:
I start with an abstract specification of CDCL and prove termination
and correctness. CDCL is specified as an abstract transition system. I
have developped two variants: one based on Nieuwenhuis et al. and a
more concrete one based on Weidenbach's version of CDCL. I show that
the latter version refines (specializes) the former.

* Pragmatic CDCL
:PROPERTIES:
:CUSTOM_ID: h:pcdcl
:END:
The PCDCL (standing for pragmatic CDCL) and IsaSAT/LLVM have not yet
been described in a publications, but have already been included in
the Isabelle formalization. It is currently used for very simple
simplifications, namely removing true clauses and (unconditionally)
false literals from clauses. This is very important to do (in
particular because in some problems from the SAT Competition several
thousands units are generated per minute), but this is already beyond
the CDCL framework from my thesis.

* Watched Literals
:PROPERTIES:
:CUSTOM_ID: h:watched-literals
:END:
First I developed a transition that includes the watched listeral
(with blocking clauses) optimization to efficiently find propagations and conflicts.

From there one, I enter the *nondeterminism refinement monad*. It is a
way to write nondeterministic programs (like the transition systems),
while still getting closer to programs (e.g., invariants and loops).


* Heuristics and Executable Code
:PROPERTIES:
:CUSTOM_ID: h:heuristics
:END:
Now comes the level where the nondeterminstic operations are replaced
by deterministic heuristics. This level is not so important from the
Isabelle point-of-view, but is absolutely critical for performance. It
is very easy to destroy performance by restarting too often (or not
often enough).

Finally I use [[https://www21.in.tum.de/~lammich/isabelle_llvm/][Peter Lammich's Isabelle-LLVM]] to generate code in LLVM's
intermediate language. The code can be compiled with `clang`. Combined
with a hand-written parser and printer of the output, the code can be
fed with a DIMACS like any other SAT solver.

During my PhD thesis I used the (older) Refinement Framework to generate
Standard ML. This allowed me to prove completeness (for a
powerful-enough compiler and machine), but the code was twice as slow
as the LLVM-generated version (same Isabelle code!) -- and in the end
the completeness guarantees were similar.
* Correctness
:PROPERTIES:
:CUSTOM_ID: h:correctness
:END:
In essence, the completeness mentioned in the graph, means completeness unless
you cannot allocate enough memory:
  + In the Standard ML modelisation of Isabelle, arrays can be
    accessed by arbitrary sized integers. However, compilers do not
    have to support that.  Therefore, in practice, the memory is
    limited to around 2^{61} bits (for MLton) to represent the clauses
    and their headers. Such machines currently do not exist (and
    filling so much memory takes a very long time too).
  + In LLVM, we are restricted to a certain number of clauses (it must
    fit in a 64-bit integer!) and all clauses have fit in array (like
    in standard ML with the limit of 2^{63} bits). With enough memory
    (more than 2^{63} bits) and enough time, the answer could become
    "UNKNOWN" (but no overflow will lead to a wrong error!)

* Release
:PROPERTIES:
:CUSTOM_ID: h:release
:END:
** EDA-Challenge 2021
:PROPERTIES:
:CUSTOM_ID: h:2021-eda-challenge
:END:
The first official release is [[../src/isasat/isasat-eda2021-edcd29d.tar.xz][available]]. It is the version submitted
to the [[https://www.eda-ai.org/][EDA-Challenge 2021]]. To the surprise of everyone (including the
author!), it actually won the challenge. The system description
submitted to the EDA Challenge is [[http://fmv.jku.at/papers/Fleury-EDA-Challenge-2021.pdf][available]].

How good is the solver really? Another comparison is
the well-known [[https://satcompetition.github.io/][SAT Competition]] that features hard instances with long
running time. The comparison is unfair towards my SAT solver (because
you need inprocessing nowadays to even have a chance for most
instances), but the SAT Competition is the standard way to compare
solvers.

Here is a CDF with kissat (who finished second) and the solver who
finished last during the SAT Competition 2021 (in the plot, higher is better!):

[[file:../figures/sat/sc2021-isasat-kissat-cleanmaple.png]]

(Better [[file:../figures/sat/sc2021-isasat-kissat-cleanmaple.pdf][PDF]] version). The conclusion is that IsaSAT would clearly not
have won the SAT Competition 2021, but also would probably not have
been the slowest solver (if I had been able to make IsaSAT work on
Starexec). It is not a great result, but it is an indication how
far you can go without inprocessing.


|                          |           | Solved |       |         |
| Solver                   | SAT+UNSAT |    SAT | UNSAT |   PAR-2 |
|--------------------------+-----------+--------+-------+---------|
| Kissat                   |       283 |    137 |   146 | 1377523 |
| IsaSAT                   |       137 |     69 |    68 | 2795128 |
| CleanMaple\under{}PriPro |       136 |     71 |    65 | 2868588 |

The conclusion is three-fold. First, it is possible to verify
efficient code in a proof assistant. Second, there is no reason to
believe that verified code is slower than hand-written one -- even if
it takes much more time to prove correctness. Third, the perfect SAT
solver depends on the benchmarks you have.