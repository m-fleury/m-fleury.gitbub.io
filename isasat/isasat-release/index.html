<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width">
		

		<title>IsaSAT Releases</title>

		
		<link rel="stylesheet" href="https://m-fleury.github.io/css/colors-dark.min.91821db55d8acc5b9f936944224ab03a094cea2c707bacc60689050c84476d08.css">

		
	</head>
	<body>
		<header id="header">
			<h1><a href="https://m-fleury.github.io/">Webpage of Mathias Fleury</a></h1>
			<p></p>
		</header>

		<div id="page">
			<div id="sidebar">
				<nav>
	
		<ul class="nav">
			
				<li><a href="/duties/"><span>About me</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/blog/"><span>Blog</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/isasat/"><span>IsaSATs</span></a></li>
			
				<li><a href="/pasteque/"><span>Pasteques</span></a></li>
			
				<li><a href="/sources/"><span>Sources</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/sources/sources/"><span>Programs</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/research/"><span>My Research</span></a></li>
			
		</ul>
	
</nav>

			</div>

			<div id="content">
				
	<article class="post">
		<h1><a href="https://m-fleury.github.io/isasat/isasat-release/">IsaSAT Releases</a> </h1>

		<div class="post-content"><h2 id="h:2023-sc2023">SAT Competition 2023</h2>
<p>This year, once again, I submitted IsaSAT to the SAT Competition 2023
(the source code is as always <a href="../../src/isasat/isasat-sc2023.zip">available</a>). According to the results, I
did not finish last. However, compared to other solvers, I am trailing
in the unsatisfiable instances. I believe that this is due to the fact
that I still do not have bounded variable elimination. In the past, I
assumed that vivification was also very important, but its importance
is now less clear after Daniela and I attempted to examine it more
precisely for POS'23 on the SAT solver CaDiCaL.</p>
<p>Something special happened this year: it turned out that my DRAT
proofs are broken because the proof format is not strong enough to
express what I want. I was aware of this problem before the
competition (as it already existed last year).</p>
<p>My solver has two sets of clauses: the irredundant set N and the
redundant set U, with the strict invariant N⊨U. To handle pure
literals, I simply check for literals that appear only in one
direction in N (all positive or all negative) and set the
corresponding unit. For example, if N is A∨B, A∨¬B, B∨C, IsaSAT would
first learn the unit clause A, delete all clauses containing A, and
then learn the unit clauses B and C. This results in an empty problem.</p>
<p>This is legal (and is part of my proofs!). Technically, adding the
units is a DRAT clause&hellip; but only if the negated literal does not
appear in U. Given my strict invariants, I don&rsquo;t care in
IsaSAT. However, the DRAT checker has a different opinion on this
because N and U cannot be distinguished (I have been told that VeriPB
is able to do so, but did not look at it yet). Therefore, some of my
proofs were rightfully rejected while IsaSAT did nothing incorrect and
outputted the correct answer.</p>
<p>Other SAT solvers also face this issue, but they simply remove all the
clauses containing a pure literal (both in N and U), making the
transformation legal – but maybe a clause in U could have been useful
for solving the problem.</p>
<p>Currently, there is no solution to this problem – execept that I only
generate proofs for the SAT Competition, because it is mandatory in
the main track, even though my SAT solver is entirely verified so the
answer cannot be wrong.</p>
<p>Interestingly I found a heuristic bug after the submission, but it
only helps on the SC2021 and SC2022 problems, not on the SC2023
problems. So this is a story for another time.</p>
<figure><img src="/ox-hugo/sc2022-heuristic-fix.png">
</figure>

<p>(sc2023 is basically the version submitted and the black one has the
fix, see the <a href="/ox-hugo/sc2022-heuristic-fix.png">PDF for a better version.</a>)</p>
<h2 id="h:2022-sc2022">SAT Competition 2022 and v2022-01</h2>
<p>The second release is now <a href="../../src/isasat/isasat-sc2022-663798f.tar.xz">available</a>. It is the version submitted to
the SAT Competition 2022. This year I managed to make IsaSAT work on
StarExec.</p>
<p>Compared to the version form the EDA Challenge:</p>
<ul>
<li>
<p>It can now produces DRAT proofs. There are absolutely no
guarantees on the correctness of those proofs&hellip; And in
particular, I have managed to produce wrong proofs when I forgot
to log unit clauses. This does not matter because the entire
solver is verified, but it is mandatory to take part in the Main
track.</p>
</li>
<li>
<p>I have implemented pure literal deletion. IsaSAT search for
literals that do not appear or appear in only one polarity and
sets them to true. It is the first transformation performed by
IsaSAT that does not preserve models.</p>
</li>
</ul>
<p>Shortly after the SAT Competition deadline, while drinking tea and
looking at the <a href="https://people.mpi-inf.mpg.de/~mfleury/IsaFoL/current/Weidenbach_Book/IsaSAT/">HTML generated documentation</a>, I found two heuristics bugs,
which are solved in the <a href="../../src/isasat/isasat-v2022_01-30966c1.tar.xz">release v2022-01</a>. According to my benchmarks it solves
23 more problems on the benchmarks from the SAT Competition 2021:</p>
<figure><img src="/ox-hugo/sc2021-isasat-schedule-fix.png">
</figure>

<table>
<thead>
<tr>
<th></th>
<th>solved</th>
<th>sat</th>
<th>uns</th>
<th>par2</th>
</tr>
</thead>
<tbody>
<tr>
<td>with-proper-scheduling</td>
<td>198</td>
<td>87</td>
<td>111</td>
<td>2273168</td>
</tr>
<tr>
<td>sc2022</td>
<td>176</td>
<td>85</td>
<td>91</td>
<td>2457296</td>
</tr>
</tbody>
</table>
<p>(Better <a href="/ox-hugo/sc2021-isasat-schedule-fix.pdf">PDF version</a> of the cactus plot). So, clearly a good fix. I
will probably try to describe to the bug in more details, but in
short: I was restarting way too little and reducing way too much. And
I did not realise that because I swapped it when I printed it during
the statistics.</p>
<h2 id="h:2021-eda-challenge">EDA-Challenge 2021</h2>
<p>The first official release is <a href="../../src/isasat/isasat-eda2021-edcd29d.tar.xz">available</a>. It is the version submitted
to the <a href="https://www.eda-ai.org/">EDA-Challenge 2021</a>. To the surprise of everyone (including the
author!), it actually won the challenge. The system description
submitted to the EDA Challenge is <a href="http://fmv.jku.at/papers/Fleury-EDA-Challenge-2021.pdf">available</a>.</p>
<p>How good is the solver really? Another comparison is
the well-known <a href="https://satcompetition.github.io/">SAT Competition</a> that features hard instances with long
running time. The comparison is unfair towards my SAT solver (because
you need inprocessing nowadays to even have a chance for most
instances), but the SAT Competition is the standard way to compare
solvers.</p>
<p>Here is a CDF with kissat (who finished second) and the solver who
finished last during the SAT Competition 2021 (in the plot, higher is better!):</p>
<figure><img src="/ox-hugo/sc2021-isasat-kissat-cleanmaple.png">
</figure>

<p>(Better <a href="/ox-hugo/sc2021-isasat-kissat-cleanmaple.pdf">PDF</a> version). The conclusion is that IsaSAT would clearly not
have won the SAT Competition 2021, but also would probably not have
been the slowest solver (if I had been able to make IsaSAT work on
Starexec). It is not a great result, but it is an indication how
far you can go without inprocessing.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Solved</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Solver</td>
<td>SAT+UNSAT</td>
<td>SAT</td>
<td>UNSAT</td>
<td>PAR-2</td>
</tr>
<tr>
<td>Kissat</td>
<td>283</td>
<td>137</td>
<td>146</td>
<td>1377523</td>
</tr>
<tr>
<td>IsaSAT</td>
<td>137</td>
<td>69</td>
<td>68</td>
<td>2795128</td>
</tr>
<tr>
<td>CleanMaple_PriPro</td>
<td>136</td>
<td>71</td>
<td>65</td>
<td>2868588</td>
</tr>
</tbody>
</table>
<p>The conclusion is three-fold. First, it is possible to verify
efficient code in a proof assistant. Second, there is no reason to
believe that verified code is slower than hand-written
one<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> &ndash; even if it takes much more time to prove
correctness. Third, the perfect SAT solver depends on the benchmarks
you have.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I don&rsquo;t believe that for verified code targetting
Standard ML, however. Even if MLton is a very impressive compiler for
Standard ML code.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div>

		<p class="meta">Posted on <span class="postdate">22. July 2023</span></p>
	</article>

			</div>

			<footer id="footer">
				<p class="copyright">
					
						Powered by <a href="https://gohugo.io/">Hugo</a> and the
						<a href="https://github.com/bake/solar-theme-hugo">Solar</a>-theme.
					
				</p>
			</footer>
		</div>

		
	</body>
</html>
